{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1wz-ud5Hav1O",
    "colab_type": "code",
    "outputId": "04ec5035-f1a0-4486-bea2-6a03ec819411",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139.0
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.588925680906E12,
     "user_tz": -180.0,
     "elapsed": 25301.0,
     "user": {
      "displayName": "dror bar",
      "photoUrl": "",
      "userId": "08597478424783230611"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n",
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vjO8Tu5MbI-0",
    "colab_type": "code",
    "outputId": "fcebbdd2-2c84-462c-8541-67f4eb1be23d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.588925693407E12,
     "user_tz": -180.0,
     "elapsed": 1790.0,
     "user": {
      "displayName": "dror bar",
      "photoUrl": "",
      "userId": "08597478424783230611"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/nn\n"
     ]
    }
   ],
   "source": [
    "cd /gdrive/My\\ Drive/nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5BI8sNFMb0Lp",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    },
    "outputId": "0cd07b8b-ac4e-4278-b634-ff2abfa3070f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.588925703035E12,
     "user_tz": -180.0,
     "elapsed": 7671.0,
     "user": {
      "displayName": "dror bar",
      "photoUrl": "",
      "userId": "08597478424783230611"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)  # for reproducibility\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_random_seed(5005)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
    "from tensorflow.python.keras.layers.pooling import MaxPooling1D\n",
    "from tensorflow.python.keras.layers.pooling import AveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow.python.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.use('Agg')\n",
    "import utils\n",
    "sys.path.append(\".\")\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "from utils import precision, recall_TP, load_data_merged, recall_TN, precision_N\n",
    "\n",
    "l2_lam = 5e-07 \n",
    "l1_lam = 1e-08 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "lHcO6wobeAjx",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def train_diff_model(data_path, res_path, model_name, input_len,\n",
    "                     num_epoch, batchsize, func,model_path=\"./weights.hdf5\", \n",
    "                     number_of_folds=1, save=True):\n",
    "    print('creating model')\n",
    "    model = func(input_len)\n",
    "    print('compiling model')\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', recall_TP,recall_TN])\n",
    "    checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    print('loading data')\n",
    "    x_train_list, y_train_list, x_valid_list, y_valid_list, x_test_seq, y_test = load_data_merged(data_path, input_len, kfold=number_of_folds)\n",
    "    \n",
    "    print(\"calculating weights\")\n",
    "    if number_of_folds != 1:\n",
    "      print(\"the class weights won't work\")\n",
    "\n",
    "    print('fitting the model')\n",
    "    for i in range(len(x_train_list)):\n",
    "      print(\"Using fold %s/%s\" %(i+1, number_of_folds))\n",
    "      x_train_seq = x_train_list[i]\n",
    "      y_train = y_train_list[i]\n",
    "      x_valid_seq = x_valid_list[i]\n",
    "      y_valid = y_valid_list[i]\n",
    "      sample_weights = class_weight.compute_sample_weight('balanced', y_train)\n",
    "\n",
    "\n",
    "      history = model.fit(x_train_seq, y_train, epochs=num_epoch, batch_size=batchsize,\n",
    "                          validation_data=(x_valid_seq, y_valid), verbose=1,\n",
    "                          callbacks=[checkpointer, earlystopper, ], sample_weight=sample_weights)  # tb])\n",
    "    \n",
    "    print(\"Finish training\")\n",
    "    if save:\n",
    "      print('saving the model')\n",
    "      model.save(os.path.join(res_path, model_name + \".h5\"))\n",
    "\n",
    "    print('testing the model')\n",
    "    score = model.evaluate(x_test_seq, y_test)\n",
    "\n",
    "    for i in range(len(model.metrics_names)):\n",
    "        print(str(model.metrics_names[i]) + \": \" + str(score[i]))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-eT9wSZvN8yA",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "outputId": "c479d355-700a-46ea-e49b-992f216f1067",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.588937599383E12,
     "user_tz": -180.0,
     "elapsed": 776360.0,
     "user": {
      "displayName": "dror bar",
      "photoUrl": "",
      "userId": "08597478424783230611"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 150, 4)]          0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv1D)               (None, 150, 90)           450       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 148, 90)           24390     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling1D)         (None, 147, 90)           0         \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 147, 90)           0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 143, 100)          45100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 142, 100)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 142, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 14200)             0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 500)               7100500   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 7,295,941\n",
      "Trainable params: 7,295,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "compiling model\n",
      "loading data\n",
      "calculating weights\n",
      "fitting the model\n",
      "Using fold 1/1\n",
      "Train on 583680 samples, validate on 145920 samples\n",
      "Epoch 1/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.5851 - acc: 0.6930 - recall_TP: 0.6998 - recall_TN: 0.6898\n",
      "Epoch 00001: val_loss improved from inf to 0.55083, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 54s 93us/sample - loss: 0.5851 - acc: 0.6930 - recall_TP: 0.6998 - recall_TN: 0.6898 - val_loss: 0.5508 - val_acc: 0.7203 - val_recall_TP: 0.7278 - val_recall_TN: 0.7164\n",
      "Epoch 2/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.5627 - acc: 0.7122 - recall_TP: 0.7297 - recall_TN: 0.7037\n",
      "Epoch 00002: val_loss improved from 0.55083 to 0.53578, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 54s 92us/sample - loss: 0.5627 - acc: 0.7122 - recall_TP: 0.7297 - recall_TN: 0.7037 - val_loss: 0.5358 - val_acc: 0.7258 - val_recall_TP: 0.7405 - val_recall_TN: 0.7186\n",
      "Epoch 3/20\n",
      "583424/583680 [============================>.] - ETA: 0s - loss: 0.5530 - acc: 0.7186 - recall_TP: 0.7433 - recall_TN: 0.7065\n",
      "Epoch 00003: val_loss did not improve from 0.53578\n",
      "583680/583680 [==============================] - 53s 91us/sample - loss: 0.5529 - acc: 0.7186 - recall_TP: 0.7433 - recall_TN: 0.7065 - val_loss: 0.5478 - val_acc: 0.7352 - val_recall_TP: 0.7264 - val_recall_TN: 0.7395\n",
      "Epoch 4/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.7249 - recall_TP: 0.7516 - recall_TN: 0.7120\n",
      "Epoch 00004: val_loss improved from 0.53578 to 0.53521, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 54s 92us/sample - loss: 0.5437 - acc: 0.7249 - recall_TP: 0.7516 - recall_TN: 0.7120 - val_loss: 0.5352 - val_acc: 0.7294 - val_recall_TP: 0.7584 - val_recall_TN: 0.7149\n",
      "Epoch 5/20\n",
      "583168/583680 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7327 - recall_TP: 0.7597 - recall_TN: 0.7197\n",
      "Epoch 00005: val_loss improved from 0.53521 to 0.53125, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 54s 93us/sample - loss: 0.5324 - acc: 0.7327 - recall_TP: 0.7596 - recall_TN: 0.7197 - val_loss: 0.5312 - val_acc: 0.7366 - val_recall_TP: 0.7424 - val_recall_TN: 0.7339\n",
      "Epoch 6/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.7432 - recall_TP: 0.7653 - recall_TN: 0.7323\n",
      "Epoch 00006: val_loss improved from 0.53125 to 0.52343, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 54s 92us/sample - loss: 0.5190 - acc: 0.7432 - recall_TP: 0.7653 - recall_TN: 0.7323 - val_loss: 0.5234 - val_acc: 0.7427 - val_recall_TP: 0.7288 - val_recall_TN: 0.7496\n",
      "Epoch 7/20\n",
      "583552/583680 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.7521 - recall_TP: 0.7747 - recall_TN: 0.7411\n",
      "Epoch 00007: val_loss did not improve from 0.52343\n",
      "583680/583680 [==============================] - 53s 91us/sample - loss: 0.5056 - acc: 0.7521 - recall_TP: 0.7747 - recall_TN: 0.7411 - val_loss: 0.5445 - val_acc: 0.7295 - val_recall_TP: 0.7754 - val_recall_TN: 0.7071\n",
      "Epoch 8/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.7623 - recall_TP: 0.7821 - recall_TN: 0.7527\n",
      "Epoch 00008: val_loss improved from 0.52343 to 0.51631, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 54s 92us/sample - loss: 0.4918 - acc: 0.7624 - recall_TP: 0.7822 - recall_TN: 0.7527 - val_loss: 0.5163 - val_acc: 0.7494 - val_recall_TP: 0.7115 - val_recall_TN: 0.7676\n",
      "Epoch 9/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.7713 - recall_TP: 0.7907 - recall_TN: 0.7619\n",
      "Epoch 00009: val_loss did not improve from 0.51631\n",
      "583680/583680 [==============================] - 53s 91us/sample - loss: 0.4776 - acc: 0.7713 - recall_TP: 0.7907 - recall_TN: 0.7619 - val_loss: 0.5350 - val_acc: 0.7364 - val_recall_TP: 0.7584 - val_recall_TN: 0.7256\n",
      "Epoch 10/20\n",
      "583424/583680 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.7799 - recall_TP: 0.7989 - recall_TN: 0.7706\n",
      "Epoch 00010: val_loss did not improve from 0.51631\n",
      "583680/583680 [==============================] - 53s 91us/sample - loss: 0.4646 - acc: 0.7798 - recall_TP: 0.7989 - recall_TN: 0.7706 - val_loss: 0.5281 - val_acc: 0.7442 - val_recall_TP: 0.7361 - val_recall_TN: 0.7483\n",
      "Epoch 11/20\n",
      "583552/583680 [============================>.] - ETA: 0s - loss: 0.4534 - acc: 0.7860 - recall_TP: 0.8042 - recall_TN: 0.7772\n",
      "Epoch 00011: val_loss did not improve from 0.51631\n",
      "583680/583680 [==============================] - 53s 90us/sample - loss: 0.4534 - acc: 0.7860 - recall_TP: 0.8042 - recall_TN: 0.7772 - val_loss: 0.5311 - val_acc: 0.7408 - val_recall_TP: 0.7486 - val_recall_TN: 0.7372\n",
      "Epoch 12/20\n",
      "583040/583680 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.7943 - recall_TP: 0.8123 - recall_TN: 0.7854\n",
      "Epoch 00012: val_loss did not improve from 0.51631\n",
      "583680/583680 [==============================] - 53s 90us/sample - loss: 0.4401 - acc: 0.7942 - recall_TP: 0.8124 - recall_TN: 0.7854 - val_loss: 0.5230 - val_acc: 0.7439 - val_recall_TP: 0.7386 - val_recall_TN: 0.7463\n",
      "Epoch 13/20\n",
      "583552/583680 [============================>.] - ETA: 0s - loss: 0.4289 - acc: 0.8006 - recall_TP: 0.8193 - recall_TN: 0.7915\n",
      "Epoch 00013: val_loss did not improve from 0.51631\n",
      "583680/583680 [==============================] - 53s 90us/sample - loss: 0.4289 - acc: 0.8006 - recall_TP: 0.8193 - recall_TN: 0.7915 - val_loss: 0.5298 - val_acc: 0.7431 - val_recall_TP: 0.7402 - val_recall_TN: 0.7444\n",
      "Epoch 00013: early stopping\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "model1 = train_diff_model(data_path=r\"dataset/test.pkl\", \n",
    "                 res_path=\"./models\", model_name=\"150cpg\", model_path=\"./models/temp/1\",\n",
    "                 input_len=150, num_epoch=20, batchsize=128, number_of_folds=1,save=False, func = model26_another_chance_for_2nd_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2OjBzUHOUL7Y",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "outputId": "63c12543-45ee-4807-c7a0-9ebcd87faadc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.588938196164E12,
     "user_tz": -180.0,
     "elapsed": 596747.0,
     "user": {
      "displayName": "dror bar",
      "photoUrl": "",
      "userId": "08597478424783230611"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 150, 4)]          0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv1D)               (None, 150, 90)           450       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 148, 90)           24390     \n",
      "_________________________________________________________________\n",
      "pool1 (AveragePooling1D)     (None, 147, 90)           0         \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 147, 90)           0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 143, 100)          45100     \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 142, 100)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 142, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 14200)             0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 500)               7100500   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 7,170,941\n",
      "Trainable params: 7,170,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "compiling model\n",
      "loading data\n",
      "calculating weights\n",
      "fitting the model\n",
      "Using fold 1/1\n",
      "Train on 583680 samples, validate on 145920 samples\n",
      "Epoch 1/20\n",
      "583168/583680 [============================>.] - ETA: 0s - loss: 0.5811 - acc: 0.6954 - recall_TP: 0.7064 - recall_TN: 0.6898\n",
      "Epoch 00001: val_loss improved from inf to 0.54848, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 53s 90us/sample - loss: 0.5811 - acc: 0.6954 - recall_TP: 0.7064 - recall_TN: 0.6898 - val_loss: 0.5485 - val_acc: 0.7173 - val_recall_TP: 0.7405 - val_recall_TN: 0.7059\n",
      "Epoch 2/20\n",
      "582912/583680 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.7145 - recall_TP: 0.7317 - recall_TN: 0.7062\n",
      "Epoch 00002: val_loss did not improve from 0.54848\n",
      "583680/583680 [==============================] - 51s 88us/sample - loss: 0.5596 - acc: 0.7145 - recall_TP: 0.7317 - recall_TN: 0.7062 - val_loss: 0.5517 - val_acc: 0.7254 - val_recall_TP: 0.7480 - val_recall_TN: 0.7142\n",
      "Epoch 3/20\n",
      "583552/583680 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.7225 - recall_TP: 0.7400 - recall_TN: 0.7140\n",
      "Epoch 00003: val_loss improved from 0.54848 to 0.54234, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 52s 90us/sample - loss: 0.5489 - acc: 0.7225 - recall_TP: 0.7399 - recall_TN: 0.7140 - val_loss: 0.5423 - val_acc: 0.7334 - val_recall_TP: 0.7355 - val_recall_TN: 0.7326\n",
      "Epoch 4/20\n",
      "583040/583680 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7312 - recall_TP: 0.7480 - recall_TN: 0.7230\n",
      "Epoch 00004: val_loss did not improve from 0.54234\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.5361 - acc: 0.7312 - recall_TP: 0.7480 - recall_TN: 0.7230 - val_loss: 0.5450 - val_acc: 0.7318 - val_recall_TP: 0.7483 - val_recall_TN: 0.7240\n",
      "Epoch 5/20\n",
      "583552/583680 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.7440 - recall_TP: 0.7584 - recall_TN: 0.7370\n",
      "Epoch 00005: val_loss improved from 0.54234 to 0.53565, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 51s 88us/sample - loss: 0.5175 - acc: 0.7440 - recall_TP: 0.7584 - recall_TN: 0.7370 - val_loss: 0.5357 - val_acc: 0.7390 - val_recall_TP: 0.7341 - val_recall_TN: 0.7415\n",
      "Epoch 6/20\n",
      "583168/583680 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.7605 - recall_TP: 0.7746 - recall_TN: 0.7538\n",
      "Epoch 00006: val_loss did not improve from 0.53565\n",
      "583680/583680 [==============================] - 51s 88us/sample - loss: 0.4917 - acc: 0.7605 - recall_TP: 0.7747 - recall_TN: 0.7537 - val_loss: 0.5506 - val_acc: 0.7294 - val_recall_TP: 0.7604 - val_recall_TN: 0.7140\n",
      "Epoch 7/20\n",
      "583424/583680 [============================>.] - ETA: 0s - loss: 0.4640 - acc: 0.7787 - recall_TP: 0.7906 - recall_TN: 0.7728\n",
      "Epoch 00007: val_loss did not improve from 0.53565\n",
      "583680/583680 [==============================] - 51s 88us/sample - loss: 0.4640 - acc: 0.7787 - recall_TP: 0.7906 - recall_TN: 0.7728 - val_loss: 0.5497 - val_acc: 0.7317 - val_recall_TP: 0.7488 - val_recall_TN: 0.7234\n",
      "Epoch 8/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.7956 - recall_TP: 0.8061 - recall_TN: 0.7904\n",
      "Epoch 00008: val_loss did not improve from 0.53565\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.4366 - acc: 0.7956 - recall_TP: 0.8061 - recall_TN: 0.7904 - val_loss: 0.5536 - val_acc: 0.7346 - val_recall_TP: 0.7354 - val_recall_TN: 0.7343\n",
      "Epoch 9/20\n",
      "582912/583680 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8103 - recall_TP: 0.8218 - recall_TN: 0.8048\n",
      "Epoch 00009: val_loss did not improve from 0.53565\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.4108 - acc: 0.8103 - recall_TP: 0.8217 - recall_TN: 0.8048 - val_loss: 0.5558 - val_acc: 0.7387 - val_recall_TP: 0.7058 - val_recall_TN: 0.7550\n",
      "Epoch 10/20\n",
      "583168/583680 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8245 - recall_TP: 0.8343 - recall_TN: 0.8197\n",
      "Epoch 00010: val_loss did not improve from 0.53565\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.3864 - acc: 0.8245 - recall_TP: 0.8343 - recall_TN: 0.8197 - val_loss: 0.5752 - val_acc: 0.7320 - val_recall_TP: 0.7379 - val_recall_TN: 0.7288\n",
      "Epoch 00010: early stopping\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "model2 = train_diff_model(data_path=r\"dataset/test.pkl\", \n",
    "                 res_path=\"./models\", model_name=\"150cpg\", model_path=\"./models/temp/1\",\n",
    "                 input_len=150, num_epoch=20, batchsize=128, number_of_folds=1,save=False, func = model26_avg_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pUXD06JkUL5K",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "outputId": "5300b075-42eb-4867-c609-239e796cad9f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.588938943182E12,
     "user_tz": -180.0,
     "elapsed": 1343738.0,
     "user": {
      "displayName": "dror bar",
      "photoUrl": "",
      "userId": "08597478424783230611"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 150, 4)]          0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv1D)               (None, 150, 90)           450       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 148, 90)           24390     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling1D)         (None, 147, 90)           0         \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 147, 90)           0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 143, 100)          45100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 142, 100)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 142, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 14200)             0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 500)               7100500   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 7,170,941\n",
      "Trainable params: 7,170,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "compiling model\n",
      "loading data\n",
      "calculating weights\n",
      "fitting the model\n",
      "Using fold 1/1\n",
      "Train on 583680 samples, validate on 145920 samples\n",
      "Epoch 1/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.6950 - recall_TP: 0.7000 - recall_TN: 0.6925\n",
      "Epoch 00001: val_loss improved from inf to 0.54686, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 52s 90us/sample - loss: 0.5827 - acc: 0.6950 - recall_TP: 0.7000 - recall_TN: 0.6925 - val_loss: 0.5469 - val_acc: 0.7281 - val_recall_TP: 0.7051 - val_recall_TN: 0.7397\n",
      "Epoch 2/20\n",
      "583040/583680 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.7134 - recall_TP: 0.7322 - recall_TN: 0.7043\n",
      "Epoch 00002: val_loss did not improve from 0.54686\n",
      "583680/583680 [==============================] - 51s 88us/sample - loss: 0.5597 - acc: 0.7133 - recall_TP: 0.7322 - recall_TN: 0.7042 - val_loss: 0.5529 - val_acc: 0.7209 - val_recall_TP: 0.7535 - val_recall_TN: 0.7050\n",
      "Epoch 3/20\n",
      "583424/583680 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.7200 - recall_TP: 0.7419 - recall_TN: 0.7093\n",
      "Epoch 00003: val_loss did not improve from 0.54686\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.5508 - acc: 0.7200 - recall_TP: 0.7419 - recall_TN: 0.7093 - val_loss: 0.5500 - val_acc: 0.7296 - val_recall_TP: 0.7485 - val_recall_TN: 0.7205\n",
      "Epoch 4/20\n",
      "583424/583680 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7269 - recall_TP: 0.7480 - recall_TN: 0.7165\n",
      "Epoch 00004: val_loss did not improve from 0.54686\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.5406 - acc: 0.7269 - recall_TP: 0.7480 - recall_TN: 0.7165 - val_loss: 0.5478 - val_acc: 0.7248 - val_recall_TP: 0.7762 - val_recall_TN: 0.6993\n",
      "Epoch 5/20\n",
      "583424/583680 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7342 - recall_TP: 0.7560 - recall_TN: 0.7237\n",
      "Epoch 00005: val_loss improved from 0.54686 to 0.52710, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 52s 89us/sample - loss: 0.5292 - acc: 0.7342 - recall_TP: 0.7560 - recall_TN: 0.7238 - val_loss: 0.5271 - val_acc: 0.7406 - val_recall_TP: 0.7214 - val_recall_TN: 0.7501\n",
      "Epoch 6/20\n",
      "583552/583680 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7441 - recall_TP: 0.7648 - recall_TN: 0.7340\n",
      "Epoch 00006: val_loss did not improve from 0.52710\n",
      "583680/583680 [==============================] - 51s 88us/sample - loss: 0.5148 - acc: 0.7441 - recall_TP: 0.7648 - recall_TN: 0.7340 - val_loss: 0.5531 - val_acc: 0.7221 - val_recall_TP: 0.7772 - val_recall_TN: 0.6951\n",
      "Epoch 7/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.7535 - recall_TP: 0.7741 - recall_TN: 0.7435\n",
      "Epoch 00007: val_loss did not improve from 0.52710\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.4991 - acc: 0.7535 - recall_TP: 0.7741 - recall_TN: 0.7435 - val_loss: 0.5518 - val_acc: 0.7259 - val_recall_TP: 0.7697 - val_recall_TN: 0.7045\n",
      "Epoch 8/20\n",
      "583040/583680 [============================>.] - ETA: 0s - loss: 0.4809 - acc: 0.7656 - recall_TP: 0.7852 - recall_TN: 0.7560\n",
      "Epoch 00008: val_loss improved from 0.52710 to 0.52463, saving model to ./models/temp/1\n",
      "583680/583680 [==============================] - 52s 89us/sample - loss: 0.4810 - acc: 0.7655 - recall_TP: 0.7851 - recall_TN: 0.7560 - val_loss: 0.5246 - val_acc: 0.7454 - val_recall_TP: 0.7058 - val_recall_TN: 0.7648\n",
      "Epoch 9/20\n",
      "583168/583680 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.7787 - recall_TP: 0.7962 - recall_TN: 0.7702\n",
      "Epoch 00009: val_loss did not improve from 0.52463\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.4611 - acc: 0.7786 - recall_TP: 0.7961 - recall_TN: 0.7702 - val_loss: 0.5298 - val_acc: 0.7422 - val_recall_TP: 0.7145 - val_recall_TN: 0.7554\n",
      "Epoch 10/20\n",
      "583424/583680 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.7913 - recall_TP: 0.8063 - recall_TN: 0.7841\n",
      "Epoch 00010: val_loss did not improve from 0.52463\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.4402 - acc: 0.7913 - recall_TP: 0.8063 - recall_TN: 0.7841 - val_loss: 0.5410 - val_acc: 0.7420 - val_recall_TP: 0.7206 - val_recall_TN: 0.7523\n",
      "Epoch 11/20\n",
      "583424/583680 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.8037 - recall_TP: 0.8171 - recall_TN: 0.7971\n",
      "Epoch 00011: val_loss did not improve from 0.52463\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.4205 - acc: 0.8037 - recall_TP: 0.8171 - recall_TN: 0.7971 - val_loss: 0.5463 - val_acc: 0.7435 - val_recall_TP: 0.7120 - val_recall_TN: 0.7591\n",
      "Epoch 12/20\n",
      "583296/583680 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8155 - recall_TP: 0.8267 - recall_TN: 0.8099\n",
      "Epoch 00012: val_loss did not improve from 0.52463\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.4002 - acc: 0.8155 - recall_TP: 0.8267 - recall_TN: 0.8100 - val_loss: 0.5394 - val_acc: 0.7466 - val_recall_TP: 0.6979 - val_recall_TN: 0.7708\n",
      "Epoch 13/20\n",
      "583040/583680 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8270 - recall_TP: 0.8361 - recall_TN: 0.8225\n",
      "Epoch 00013: val_loss did not improve from 0.52463\n",
      "583680/583680 [==============================] - 51s 87us/sample - loss: 0.3816 - acc: 0.8270 - recall_TP: 0.8360 - recall_TN: 0.8225 - val_loss: 0.5520 - val_acc: 0.7494 - val_recall_TP: 0.6873 - val_recall_TN: 0.7798\n",
      "Epoch 00013: early stopping\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "model3 = train_diff_model(data_path=r\"dataset/test.pkl\", \n",
    "                 res_path=\"./models\", model_name=\"150cpg\", model_path=\"./models/temp/1\",\n",
    "                 input_len=150, num_epoch=20, batchsize=128, number_of_folds=1,save=False, func = model26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06GOBU5s_3Y7",
    "colab_type": "text"
   },
   "source": [
    "**Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "PDkrCgcz5PQL",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def model26_avg_pool(input_len):\n",
    "    K.clear_session()\n",
    "\n",
    "    input_node = Input(shape=(input_len, 4), name=\"input\")\n",
    "    conv0 = Conv1D(filters=90, kernel_size=1, padding='valid', activation=\"relu\", name=\"conv0\",kernel_regularizer=regularizers.l2(l2_lam))(input_node)\n",
    "    conv1 = Conv1D(filters=90, kernel_size=3, padding='valid', activation=\"relu\", name=\"conv1\",kernel_regularizer=regularizers.l2(l2_lam))(conv0)\n",
    "    pool1 = AveragePooling1D(pool_size=2, strides=1, name=\"pool1\")(conv1)\n",
    "    drop1 = Dropout(0.25, name=\"drop1\")(pool1)\n",
    "  \n",
    "    conv2 = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv2\",kernel_regularizer=regularizers.l2(l2_lam))(drop1)\n",
    "    pool2 = AveragePooling1D(pool_size=2, strides=1)(conv2)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "    flat = Flatten()(drop2)\n",
    "\n",
    "    hidden1 = Dense(500, activation='relu', name=\"hidden1\",kernel_regularizer=regularizers.l1(l1_lam))(flat)\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(hidden1)\n",
    "    model = Model(inputs=[input_node], outputs=output)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5HBP2NCg5PNh",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def model26(input_len):\n",
    "    K.clear_session()\n",
    "    # tf.random.set_random_seed(5005)\n",
    "\n",
    "    input_node = Input(shape=(input_len, 4), name=\"input\")\n",
    "    conv0 = Conv1D(filters=90, kernel_size=1, padding='valid', activation=\"relu\", name=\"conv0\",kernel_regularizer=regularizers.l2(l2_lam))(input_node)\n",
    "    conv1 = Conv1D(filters=90, kernel_size=3, padding='valid', activation=\"relu\", name=\"conv1\",kernel_regularizer=regularizers.l2(l2_lam))(conv0)\n",
    "    pool1 = MaxPooling1D(pool_size=2, strides=1, name=\"pool1\")(conv1)\n",
    "    drop1 = Dropout(0.25, name=\"drop1\")(pool1)\n",
    "  \n",
    "    conv2 = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv2\",kernel_regularizer=regularizers.l2(l2_lam))(drop1)\n",
    "    pool2 = MaxPooling1D(pool_size=2, strides=1)(conv2)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "    flat = Flatten()(drop2)\n",
    "\n",
    "    hidden1 = Dense(500, activation='relu', name=\"hidden1\",kernel_regularizer=regularizers.l1(l1_lam))(flat)\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(hidden1)\n",
    "    model = Model(inputs=[input_node], outputs=output)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Igcrg62fzgHo",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def model26_another_chance_for_2nd_dense(input_len):\n",
    "    K.clear_session()\n",
    "    # tf.random.set_random_seed(5005)\n",
    "\n",
    "    input_node = Input(shape=(input_len, 4), name=\"input\")\n",
    "    conv0 = Conv1D(filters=90, kernel_size=1, padding='valid', activation=\"relu\", name=\"conv0\",kernel_regularizer=regularizers.l2(l2_lam))(input_node)\n",
    "    conv1 = Conv1D(filters=90, kernel_size=3, padding='valid', activation=\"relu\", name=\"conv1\",kernel_regularizer=regularizers.l2(l2_lam))(conv0)\n",
    "    pool1 = MaxPooling1D(pool_size=2, strides=1, name=\"pool1\")(conv1)\n",
    "    drop1 = Dropout(0.25, name=\"drop1\")(pool1)\n",
    "  \n",
    "    conv2 = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv2\", kernel_regularizer=regularizers.l2(l2_lam))(drop1)\n",
    "    pool2 = MaxPooling1D(pool_size=2, strides=1)(conv2)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "    flat = Flatten()(drop2)\n",
    "\n",
    "    hidden1 = Dense(500, activation='relu', name=\"hidden1\",kernel_regularizer=regularizers.l1(l1_lam))(flat)\n",
    "    drop3 = Dropout(0.25)(hidden1)\n",
    "    hidden2 = Dense(250, activation='relu', name=\"hidden2\",kernel_regularizer=regularizers.l1(l1_lam))(drop3)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(hidden2)\n",
    "    model = Model(inputs=[input_node], outputs=output)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "gvS0AuwUwlnx",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def model26_another_chance_for_2nd_dense_and_avg(input_len):\n",
    "    K.clear_session()\n",
    "    # tf.random.set_random_seed(5005)\n",
    "\n",
    "    input_node = Input(shape=(input_len, 4), name=\"input\")\n",
    "    conv0 = Conv1D(filters=90, kernel_size=1, padding='valid', activation=\"relu\", name=\"conv0\",kernel_regularizer=regularizers.l2(l2_lam))(input_node)\n",
    "    conv1 = Conv1D(filters=90, kernel_size=3, padding='valid', activation=\"relu\", name=\"conv1\",kernel_regularizer=regularizers.l2(l2_lam))(conv0)\n",
    "    pool1 = AveragePooling1D(pool_size=2, strides=1, name=\"pool1\")(conv1)\n",
    "    drop1 = Dropout(0.25, name=\"drop1\")(pool1)\n",
    "  \n",
    "    conv2 = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv2\", kernel_regularizer=regularizers.l2(l2_lam))(drop1)\n",
    "    pool2 = AveragePooling1D(pool_size=2, strides=1)(conv2)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "    flat = Flatten()(drop2)\n",
    "\n",
    "    hidden1 = Dense(500, activation='relu', name=\"hidden1\",kernel_regularizer=regularizers.l1(l1_lam))(flat)\n",
    "    drop3 = Dropout(0.25)(hidden1)\n",
    "    hidden2 = Dense(250, activation='relu', name=\"hidden2\",kernel_regularizer=regularizers.l1(l1_lam))(drop3)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(hidden2)\n",
    "    model = Model(inputs=[input_node], outputs=output)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "EvwOrwS-p3m4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7Lv1OtwF_2FV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def deepripe_model(input_len):\n",
    "    K.clear_session()\n",
    "    # tf.random.set_random_seed(5005)\n",
    "\n",
    "    input_node = Input(shape=(input_len, 4), name=\"input\")\n",
    "    conv1 = Conv1D(filters=90, kernel_size=7, padding='valid', activation=\"relu\", name=\"conv1\")(input_node)\n",
    "    pool1 = MaxPooling1D(pool_size=4, strides=2, name=\"pool1\")(conv1)\n",
    "    drop1 = Dropout(0.25, name=\"drop1\")(pool1)\n",
    "\n",
    "    conv2 = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv2\")(drop1)\n",
    "    pool2  = MaxPooling1D(pool_size=10, strides=5)(conv2)\n",
    "    drop2  = Dropout(0.25)(pool2)\n",
    "    flat = Flatten()(drop2)\n",
    "\n",
    "    hidden1 = Dense(250, activation='relu', name=\"hidden1\")(flat)\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(hidden1)\n",
    "    model = Model(inputs=[input_node], outputs=output)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "V8Qiovq3_1_5",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def model_baseline(input_len):\n",
    "    K.clear_session()\n",
    "    # tf.random.set_random_seed(5005)\n",
    "\n",
    "    input_node = Input(shape=(input_len, 4), name=\"input\")\n",
    "    conv1 = Conv1D(filters=90, kernel_size=7, padding='valid', activation=\"relu\", name=\"conv1\")(input_node)\n",
    "    pool1 = MaxPooling1D(pool_size=4, strides=2, name=\"pool1\")(conv1)\n",
    "    drop1 = Dropout(0.25, name=\"drop1\")(pool1)\n",
    "  \n",
    "    conv2 = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv2\")(drop1)\n",
    "    pool2 = MaxPooling1D(pool_size=10, strides=5)(conv2)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "    flat = Flatten()(drop2)\n",
    "\n",
    "    hidden1 = Dense(500, activation='relu', name=\"hidden1\")(flat)\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(hidden1)\n",
    "    model = Model(inputs=[input_node], outputs=output)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "OH7L0Du9ppbF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TN9AfjIdppXl",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcGySCcfdXQO",
    "colab_type": "text"
   },
   "source": [
    "Play\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "run_model.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
