{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1wz-ud5Hav1O",
    "colab_type": "code",
    "outputId": "d63663cb-5863-4147-83bf-d1de574818f5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528.0
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd0a7580851e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd /gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "vjO8Tu5MbI-0",
    "colab_type": "code",
    "outputId": "dc2d9c99-7d42-496c-837f-5f495ed03b8a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/nn\n"
     ]
    }
   ],
   "source": [
    "cd /gdrive/My\\ Drive/nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5BI8sNFMb0Lp",
    "colab_type": "code",
    "outputId": "1ae8efb6-9916-4a41-f8ef-3f91af11694c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)  # for reproducibility\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_random_seed(5005)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# tf.python.control_flow_ops = tf\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
    "from tensorflow.python.keras.layers.pooling import MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow.python.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.use('Agg')\n",
    "import utils\n",
    "sys.path.append(\".\")\n",
    "from utils import precision, recall, load_data_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "lHcO6wobeAjx",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def create_seq_model(input_len):\n",
    "    \"\"\"\n",
    "    Create a sequence model\n",
    "    :param input_len: path to file (consist of train, valid and test data)\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "    # tf.random.set_seed(5005)\n",
    "    tf.random.set_random_seed(5005)\n",
    "\n",
    "    # input_node = Input(shape=(input_len, 4), name=\"input\")\n",
    "    # conv1 = Conv1D(filters=90, kernel_size=20, padding='valid', activation=\"relu\", name=\"conv1\")(input_node)\n",
    "    # pool1 = MaxPooling1D(pool_size=10, strides=4, name=\"left_pool1\")(conv1)\n",
    "    # drop1 = Dropout(0.25, name=\"left_drop1\")(pool1)\n",
    "\n",
    "    # conv_merged = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv_merged\")(\n",
    "    #         drop1)\n",
    "    # merged_pool1 = MaxPooling1D(pool_size=4, strides=5)(conv_merged)\n",
    "    # merged_drop1 = Dropout(0.25)(merged_pool1)\n",
    "\n",
    "    # conv_merged1 = Conv1D(filters=100, kernel_size=2, padding='valid', activation=\"relu\", name=\"conv_merged2\")(\n",
    "    #     merged_drop1)\n",
    "    # merged_pool2 = MaxPooling1D(pool_size=4, strides=2)(conv_merged1)\n",
    "    # merged_drop2 = Dropout(0.25)(merged_pool2)\n",
    "\n",
    "    # if input_len > 1000:\n",
    "    #     conv_merged = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv_merged\")(\n",
    "    #         drop1)\n",
    "    #     merged_pool = MaxPooling1D(pool_size=10, strides=5)(conv_merged)\n",
    "    #     merged_drop = Dropout(0.25)(merged_pool)\n",
    "    #     merged_flat = Flatten()(merged_drop)\n",
    "    # else:\n",
    "    #     merged_flat = Flatten()(merged_drop2)\n",
    "\n",
    "    input_node = Input(shape=(input_len, 4), name=\"input\")\n",
    "    conv1 = Conv1D(filters=90, kernel_size=7, padding='valid', activation=\"relu\", name=\"conv1\")(input_node)\n",
    "    pool1 = MaxPooling1D(pool_size=4, strides=2, name=\"left_pool1\")(conv1)\n",
    "    drop1 = Dropout(0.25, name=\"left_drop1\")(pool1)\n",
    "  \n",
    "    if input_len > 10:\n",
    "        conv_merged = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv_merged\")(\n",
    "            drop1)\n",
    "        merged_pool = MaxPooling1D(pool_size=10, strides=5)(conv_merged)\n",
    "        merged_drop = Dropout(0.25)(merged_pool)\n",
    "        merged_flat = Flatten()(merged_drop)\n",
    "    else:\n",
    "        merged_flat =  Flatten()(drop1) \n",
    "\n",
    "    hidden1 = Dense(1000, activation='relu', name=\"hidden1\")(merged_flat)\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(hidden1)\n",
    "    model = Model(inputs=[input_node], outputs=output)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_diff_model(data_path, res_path, model_name, input_len,\n",
    "                     num_epoch, batchsize, model_path=\"./weights.hdf5\", \n",
    "                     number_of_folds=1, save=True):\n",
    "    \"\"\"\n",
    "    Training the model\n",
    "    :param data_path: path to file (consist of train, valid and test data)\n",
    "    :param res_path:\n",
    "    :param model_name:\n",
    "    :param input_len:\n",
    "    :param num_epoch:\n",
    "    :param batchsize:\n",
    "    :param model_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('creating model')\n",
    "    model = create_seq_model(input_len)\n",
    "    print('compiling model')\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    print('loading data')\n",
    "    x_train_list, y_train_list, x_valid_list, y_valid_list, x_test_seq, y_test = load_data_merged(data_path, input_len, kfold=number_of_folds)\n",
    "\n",
    "    print('fitting the model')\n",
    "    for i in range(len(x_train_list)):\n",
    "      print(\"Using fold %s/%s\" %(i+1, number_of_folds))\n",
    "      x_train_seq = x_train_list[i]\n",
    "      y_train = y_train_list[i]\n",
    "      x_valid_seq = x_valid_list[i]\n",
    "      y_valid = y_valid_list[i]\n",
    "\n",
    "      history = model.fit(x_train_seq, y_train, epochs=num_epoch, batch_size=batchsize,\n",
    "                          validation_data=(x_valid_seq, y_valid), verbose=2,\n",
    "                          callbacks=[checkpointer, earlystopper, ])  # tb])\n",
    "\n",
    "    if save:\n",
    "      print('saving the model')\n",
    "      model.save(os.path.join(res_path, model_name + \".h5\"))\n",
    "\n",
    "    print('testing the model')\n",
    "    score = model.evaluate(x_test_seq, y_test)\n",
    "\n",
    "    for i in range(len(model.metrics_names)):\n",
    "        print(str(model.metrics_names[i]) + \": \" + str(score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0xwMTa_vbH8-",
    "colab_type": "code",
    "outputId": "d8ebf371-00a8-4af3-ca66-33cd8f4c2cf9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 150, 4)]          0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 144, 90)           2610      \n",
      "_________________________________________________________________\n",
      "left_pool1 (MaxPooling1D)    (None, 71, 90)            0         \n",
      "_________________________________________________________________\n",
      "left_drop1 (Dropout)         (None, 71, 90)            0         \n",
      "_________________________________________________________________\n",
      "conv_merged (Conv1D)         (None, 67, 100)           45100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 12, 100)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 1500)              1801500   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 1501      \n",
      "=================================================================\n",
      "Total params: 1,850,711\n",
      "Trainable params: 1,850,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "compiling model\n",
      "loading data\n",
      "fitting the model\n",
      "Using fold 1/10\n",
      "Train on 937330 samples, validate on 104148 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59363, saving model to ./models/temp/1\n",
      "937330/937330 - 37s - loss: 0.6083 - acc: 0.6709 - val_loss: 0.5936 - val_acc: 0.6920\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59363\n",
      "937330/937330 - 36s - loss: 0.5925 - acc: 0.6877 - val_loss: 0.6008 - val_acc: 0.6866\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59363 to 0.59063, saving model to ./models/temp/1\n",
      "937330/937330 - 37s - loss: 0.5898 - acc: 0.6896 - val_loss: 0.5906 - val_acc: 0.6940\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59063\n",
      "937330/937330 - 36s - loss: 0.5881 - acc: 0.6906 - val_loss: 0.5930 - val_acc: 0.6916\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59063\n",
      "937330/937330 - 36s - loss: 0.5873 - acc: 0.6914 - val_loss: 0.5921 - val_acc: 0.6911\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.59063 to 0.58983, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5862 - acc: 0.6923 - val_loss: 0.5898 - val_acc: 0.6948\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.58983 to 0.58936, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5856 - acc: 0.6933 - val_loss: 0.5894 - val_acc: 0.6953\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.58936 to 0.58644, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5848 - acc: 0.6934 - val_loss: 0.5864 - val_acc: 0.6962\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.58644 to 0.58603, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5841 - acc: 0.6938 - val_loss: 0.5860 - val_acc: 0.6973\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.58603\n",
      "937330/937330 - 36s - loss: 0.5836 - acc: 0.6946 - val_loss: 0.5948 - val_acc: 0.6945\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.58603\n",
      "937330/937330 - 36s - loss: 0.5828 - acc: 0.6949 - val_loss: 0.5885 - val_acc: 0.6941\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.58603\n",
      "937330/937330 - 36s - loss: 0.5819 - acc: 0.6955 - val_loss: 0.5895 - val_acc: 0.6948\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.58603 to 0.58587, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5817 - acc: 0.6961 - val_loss: 0.5859 - val_acc: 0.6964\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.58587 to 0.58532, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5813 - acc: 0.6960 - val_loss: 0.5853 - val_acc: 0.6973\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.58532\n",
      "937330/937330 - 36s - loss: 0.5807 - acc: 0.6964 - val_loss: 0.5857 - val_acc: 0.6985\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.58532\n",
      "937330/937330 - 35s - loss: 0.5802 - acc: 0.6970 - val_loss: 0.5863 - val_acc: 0.6974\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.58532\n",
      "937330/937330 - 36s - loss: 0.5801 - acc: 0.6968 - val_loss: 0.5880 - val_acc: 0.6980\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.58532\n",
      "937330/937330 - 35s - loss: 0.5798 - acc: 0.6975 - val_loss: 0.5862 - val_acc: 0.6980\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.58532\n",
      "937330/937330 - 35s - loss: 0.5797 - acc: 0.6972 - val_loss: 0.5893 - val_acc: 0.6949\n",
      "Epoch 00019: early stopping\n",
      "Using fold 2/10\n",
      "Train on 937330 samples, validate on 104148 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.58532 to 0.58134, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5796 - acc: 0.6979 - val_loss: 0.5813 - val_acc: 0.6991\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.58134\n",
      "937330/937330 - 35s - loss: 0.5793 - acc: 0.6976 - val_loss: 0.5876 - val_acc: 0.6987\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58134\n",
      "937330/937330 - 35s - loss: 0.5789 - acc: 0.6979 - val_loss: 0.5859 - val_acc: 0.6958\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58134\n",
      "937330/937330 - 35s - loss: 0.5786 - acc: 0.6984 - val_loss: 0.5815 - val_acc: 0.7001\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58134\n",
      "937330/937330 - 35s - loss: 0.5783 - acc: 0.6985 - val_loss: 0.5827 - val_acc: 0.6988\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58134\n",
      "937330/937330 - 35s - loss: 0.5782 - acc: 0.6984 - val_loss: 0.5883 - val_acc: 0.6971\n",
      "Epoch 00006: early stopping\n",
      "Using fold 3/10\n",
      "Train on 937330 samples, validate on 104148 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.58134 to 0.58106, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5785 - acc: 0.6980 - val_loss: 0.5811 - val_acc: 0.7024\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.58106\n",
      "937330/937330 - 35s - loss: 0.5780 - acc: 0.6988 - val_loss: 0.5812 - val_acc: 0.7016\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58106\n",
      "937330/937330 - 35s - loss: 0.5778 - acc: 0.6987 - val_loss: 0.5876 - val_acc: 0.6979\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58106\n",
      "937330/937330 - 35s - loss: 0.5778 - acc: 0.6987 - val_loss: 0.5831 - val_acc: 0.7025\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58106\n",
      "937330/937330 - 35s - loss: 0.5776 - acc: 0.6989 - val_loss: 0.5838 - val_acc: 0.6995\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58106\n",
      "937330/937330 - 35s - loss: 0.5774 - acc: 0.6988 - val_loss: 0.5831 - val_acc: 0.7027\n",
      "Epoch 00006: early stopping\n",
      "Using fold 4/10\n",
      "Train on 937330 samples, validate on 104148 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.58106 to 0.57578, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5774 - acc: 0.6993 - val_loss: 0.5758 - val_acc: 0.7074\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5773 - acc: 0.6994 - val_loss: 0.5859 - val_acc: 0.7008\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5770 - acc: 0.6997 - val_loss: 0.5804 - val_acc: 0.7042\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5768 - acc: 0.6993 - val_loss: 0.5805 - val_acc: 0.7054\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5766 - acc: 0.6995 - val_loss: 0.5839 - val_acc: 0.7022\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57578\n",
      "937330/937330 - 36s - loss: 0.5766 - acc: 0.6997 - val_loss: 0.5803 - val_acc: 0.7049\n",
      "Epoch 00006: early stopping\n",
      "Using fold 5/10\n",
      "Train on 937330 samples, validate on 104148 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.57578\n",
      "937330/937330 - 36s - loss: 0.5763 - acc: 0.7003 - val_loss: 0.5801 - val_acc: 0.7040\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5765 - acc: 0.7000 - val_loss: 0.5793 - val_acc: 0.7031\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5760 - acc: 0.7006 - val_loss: 0.5786 - val_acc: 0.7039\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5757 - acc: 0.7007 - val_loss: 0.5783 - val_acc: 0.7023\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5755 - acc: 0.7007 - val_loss: 0.5765 - val_acc: 0.7052\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5754 - acc: 0.7010 - val_loss: 0.5806 - val_acc: 0.7029\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5751 - acc: 0.7015 - val_loss: 0.5805 - val_acc: 0.7034\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.57578\n",
      "937330/937330 - 35s - loss: 0.5752 - acc: 0.7010 - val_loss: 0.5791 - val_acc: 0.7038\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.57578 to 0.57527, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5750 - acc: 0.7013 - val_loss: 0.5753 - val_acc: 0.7039\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.57527\n",
      "937330/937330 - 36s - loss: 0.5748 - acc: 0.7011 - val_loss: 0.5827 - val_acc: 0.6987\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.57527\n",
      "937330/937330 - 35s - loss: 0.5748 - acc: 0.7017 - val_loss: 0.5831 - val_acc: 0.7020\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.57527\n",
      "937330/937330 - 35s - loss: 0.5746 - acc: 0.7014 - val_loss: 0.5836 - val_acc: 0.7003\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.57527\n",
      "937330/937330 - 35s - loss: 0.5742 - acc: 0.7014 - val_loss: 0.5797 - val_acc: 0.7017\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.57527\n",
      "937330/937330 - 35s - loss: 0.5742 - acc: 0.7017 - val_loss: 0.5783 - val_acc: 0.7035\n",
      "Epoch 00014: early stopping\n",
      "Using fold 6/10\n",
      "Train on 937330 samples, validate on 104148 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.57527\n",
      "937330/937330 - 35s - loss: 0.5745 - acc: 0.7018 - val_loss: 0.5808 - val_acc: 0.7056\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57527\n",
      "937330/937330 - 35s - loss: 0.5742 - acc: 0.7014 - val_loss: 0.5792 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57527 to 0.57386, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5741 - acc: 0.7018 - val_loss: 0.5739 - val_acc: 0.7083\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57386\n",
      "937330/937330 - 35s - loss: 0.5740 - acc: 0.7022 - val_loss: 0.5822 - val_acc: 0.7071\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57386\n",
      "937330/937330 - 36s - loss: 0.5738 - acc: 0.7021 - val_loss: 0.5778 - val_acc: 0.7068\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57386\n",
      "937330/937330 - 35s - loss: 0.5738 - acc: 0.7020 - val_loss: 0.5743 - val_acc: 0.7088\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57386\n",
      "937330/937330 - 35s - loss: 0.5732 - acc: 0.7025 - val_loss: 0.5794 - val_acc: 0.7058\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.57386\n",
      "937330/937330 - 35s - loss: 0.5734 - acc: 0.7026 - val_loss: 0.5741 - val_acc: 0.7078\n",
      "Epoch 00008: early stopping\n",
      "Using fold 7/10\n",
      "Train on 937330 samples, validate on 104148 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.57386 to 0.57344, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5739 - acc: 0.7019 - val_loss: 0.5734 - val_acc: 0.7098\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57344\n",
      "937330/937330 - 35s - loss: 0.5737 - acc: 0.7023 - val_loss: 0.5813 - val_acc: 0.7046\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57344 to 0.57176, saving model to ./models/temp/1\n",
      "937330/937330 - 36s - loss: 0.5734 - acc: 0.7024 - val_loss: 0.5718 - val_acc: 0.7094\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5730 - acc: 0.7030 - val_loss: 0.5733 - val_acc: 0.7086\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57176\n",
      "937330/937330 - 36s - loss: 0.5733 - acc: 0.7023 - val_loss: 0.5784 - val_acc: 0.7054\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57176\n",
      "937330/937330 - 36s - loss: 0.5727 - acc: 0.7032 - val_loss: 0.5746 - val_acc: 0.7055\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5730 - acc: 0.7026 - val_loss: 0.5775 - val_acc: 0.7090\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5729 - acc: 0.7026 - val_loss: 0.5753 - val_acc: 0.7063\n",
      "Epoch 00008: early stopping\n",
      "Using fold 8/10\n",
      "Train on 937330 samples, validate on 104148 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5725 - acc: 0.7034 - val_loss: 0.5795 - val_acc: 0.7043\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5726 - acc: 0.7030 - val_loss: 0.5769 - val_acc: 0.7057\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5724 - acc: 0.7032 - val_loss: 0.5750 - val_acc: 0.7047\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5721 - acc: 0.7033 - val_loss: 0.5745 - val_acc: 0.7056\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5722 - acc: 0.7034 - val_loss: 0.5791 - val_acc: 0.7045\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57176\n",
      "937330/937330 - 36s - loss: 0.5717 - acc: 0.7039 - val_loss: 0.5753 - val_acc: 0.7062\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5718 - acc: 0.7039 - val_loss: 0.5783 - val_acc: 0.7053\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5718 - acc: 0.7037 - val_loss: 0.5746 - val_acc: 0.7067\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5716 - acc: 0.7035 - val_loss: 0.5736 - val_acc: 0.7062\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.57176\n",
      "937330/937330 - 36s - loss: 0.5713 - acc: 0.7041 - val_loss: 0.5799 - val_acc: 0.7038\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5717 - acc: 0.7042 - val_loss: 0.5781 - val_acc: 0.7017\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5713 - acc: 0.7041 - val_loss: 0.5827 - val_acc: 0.7026\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5714 - acc: 0.7041 - val_loss: 0.5739 - val_acc: 0.7056\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.57176\n",
      "937330/937330 - 35s - loss: 0.5714 - acc: 0.7040 - val_loss: 0.5819 - val_acc: 0.7015\n",
      "Epoch 00014: early stopping\n",
      "Using fold 9/10\n",
      "Train on 937331 samples, validate on 104147 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.57176\n",
      "937331/937331 - 36s - loss: 0.5717 - acc: 0.7037 - val_loss: 0.5756 - val_acc: 0.7091\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57176\n",
      "937331/937331 - 35s - loss: 0.5712 - acc: 0.7041 - val_loss: 0.5768 - val_acc: 0.7065\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57176\n",
      "937331/937331 - 35s - loss: 0.5715 - acc: 0.7039 - val_loss: 0.5789 - val_acc: 0.7070\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57176\n",
      "937331/937331 - 35s - loss: 0.5713 - acc: 0.7039 - val_loss: 0.5730 - val_acc: 0.7100\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57176\n",
      "937331/937331 - 35s - loss: 0.5710 - acc: 0.7041 - val_loss: 0.5756 - val_acc: 0.7086\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57176\n",
      "937331/937331 - 35s - loss: 0.5709 - acc: 0.7044 - val_loss: 0.5739 - val_acc: 0.7089\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57176\n",
      "937331/937331 - 35s - loss: 0.5709 - acc: 0.7045 - val_loss: 0.5788 - val_acc: 0.7068\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.57176\n",
      "937331/937331 - 35s - loss: 0.5705 - acc: 0.7046 - val_loss: 0.5743 - val_acc: 0.7079\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.57176\n",
      "937331/937331 - 35s - loss: 0.5705 - acc: 0.7050 - val_loss: 0.5732 - val_acc: 0.7072\n",
      "Epoch 00009: early stopping\n",
      "Using fold 10/10\n",
      "Train on 937331 samples, validate on 104147 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.57176\n",
      "937331/937331 - 36s - loss: 0.5708 - acc: 0.7043 - val_loss: 0.5732 - val_acc: 0.7068\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "source": [
    "train_diff_model(data_path=r\"dataset/classifier_data_ccpg1.pkl\", \n",
    "                 res_path=\"./models\", model_name=\"150cpg\", model_path=\"./models/temp/1\",\n",
    "                 input_len=150, num_epoch=20, batchsize=128, number_of_folds=10,save=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "run_model.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
