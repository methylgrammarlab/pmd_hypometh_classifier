{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_model.ipynb","provenance":[{"file_id":"1qysG6N3-gJJVB6B7BHO3zuP-0sCnDUOr","timestamp":1596352204324},{"file_id":"1ziwyTEY4r7YvqKNegyQMKPHy1_Szf20R","timestamp":1595938496751}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d29_mG9z6PXb","colab_type":"text"},"source":["Train a model to predict complete loss of methylation or partial loss using a sequence"]},{"cell_type":"code","metadata":{"id":"1wz-ud5Hav1O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597843606071,"user_tz":-180,"elapsed":1122,"user":{"displayName":"dror bar","photoUrl":"","userId":"08597478424783230611"}},"outputId":"07cd12ba-ea5a-49a6-c563-17f3caff5f75"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vjO8Tu5MbI-0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597843606073,"user_tz":-180,"elapsed":1076,"user":{"displayName":"dror bar","photoUrl":"","userId":"08597478424783230611"}},"outputId":"32109225-4a96-4892-889a-15a82b0fdec7"},"source":["cd /gdrive/My\\ Drive/nn "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive/nn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5BI8sNFMb0Lp","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","import argparse\n","import os\n","import pickle\n","import sys\n","import glob\n","\n","import numpy as np\n","\n","np.random.seed(7)  # for reproducibility\n","\n","import tensorflow as tf\n","tf.random.set_random_seed(5005)\n","\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.utils import class_weight\n","\n","\n","from tensorflow.python.keras.models import Model, load_model\n","from tensorflow.python.keras.layers import Input\n","from tensorflow.python.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.python.keras.layers.convolutional import Conv1D\n","from tensorflow.python.keras.layers.pooling import MaxPooling1D\n","from tensorflow.python.keras.layers.pooling import AveragePooling1D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n","import tensorflow.python.keras.backend as K\n","from keras import regularizers\n","from tensorflow.python.keras.utils import plot_model \n","\n","sys.path.append(\".\")\n","import utils\n","from utils import *\n","\n","l2_lam = 5e-07 \n","l1_lam = 1e-08 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lHcO6wobeAjx","colab_type":"code","colab":{}},"source":["def train_model_on_fold(x_train, y_train, x_test,y_test, input_len,\n","                        num_epoch, batchsize, func,model_path, class_weights, output_bias=None):\n","  \"\"\"\n","  Train a model to using the train data to predict the test data\n","  :param x_train: The train dataset \n","  :param y_train: The train labels\n","  :param x_test: The test dataset\n","  :param y_test: The test labels\n","  :param input_len: The length of the input\n","  :param num_epoch: Number of epoches \n","  :param batchsize: The batchsize \n","  :param func: The model function to use \n","  :param model_path: The path to save the model from run to run\n","  :return: The model after fitting\n","  \"\"\"\n","  model = func(input_len, output_bias=output_bias)\n","  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n","  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', recall_TP,recall_TN])\n","  checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n","  earlystopper = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1)\n","    \n","  print('fitting the model')          \n","  history = model.fit(x_train, y_train, epochs=num_epoch, batch_size=batchsize,\n","                      validation_data=(x_test, y_test), verbose=1,\n","                      callbacks=[checkpointer, earlystopper, ], class_weight=class_weights)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hy29UlwGoN35","colab_type":"code","colab":{}},"source":["def sequence_model(input_len, output_bias=None):\n","  \"\"\"\n","  Buld a model to predict a sequence information \n","  :param input_len: The length of the input\n","  \"\"\"\n","  K.clear_session()\n","  tf.random.set_random_seed(5005)\n","\n","  if output_bias:\n","    output_bias = tf.keras.initializers.Constant(output_bias)\n","\n","  input_node = Input(shape=(input_len, 4), name=\"input\")\n","  conv1 = Conv1D(filters=90, kernel_size=3, padding='valid', activation=\"relu\", name=\"conv1\",kernel_regularizer=regularizers.l2(l2_lam))(input_node)\n","  pool1 = MaxPooling1D(pool_size=2, strides=1, name=\"pool1\")(conv1)\n","  drop1 = Dropout(0.25, name=\"drop1\")(pool1)\n","\n","  conv2 = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv2\", kernel_regularizer=regularizers.l2(l2_lam))(drop1)\n","  pool2 = MaxPooling1D(pool_size=2, strides=1)(conv2)\n","  drop2 = Dropout(0.25)(pool2)\n","  flat = Flatten()(drop2)\n","\n","  hidden1 = Dense(500, activation='relu', name=\"hidden1\",kernel_regularizer=regularizers.l1(l1_lam))(flat)\n","  drop3 = Dropout(0.5)(hidden1)\n","  hidden2 = Dense(250, activation='relu', name=\"hidden2\",kernel_regularizer=regularizers.l1(l1_lam))(drop3)\n","\n","  output = Dense(1, activation='sigmoid', name=\"output\", bias_initializer=output_bias)(hidden2)\n","  model = Model(inputs=[input_node], outputs=output)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Qi9WBFs-vub","colab_type":"code","colab":{}},"source":["def train_model(data_path, model_folder=\"./models/folds_models\", temp_model_folder=\"./models/temp/weight.h5\", input_len=150, number_of_folds=3):\n","  \"\"\"\n","  Train a model or x models using the cross validation \n","  :param data_path: The path for the dataset\n","  :param model_folder: The final folder to save the models\n","  :param temp_model_folder: A folder to save the models while running\n","  :param input_len: The length of the input\n","  :param number_of_folds: Number of fold to use for the model\n","  :return: The model if we used 1 model(1 fold) or None if more \n","  \"\"\"\n","\n","  print('loading data')\n","  x_train_list, y_train_list, x_valid_list, y_valid_list, x_test_seq, y_test, x_train, y_train = load_train_validate_test_data(data_path, input_len, kfold=number_of_folds)\n","\n","  models_path = []\n","  acc_per_fold = []\n","  loss_per_fold = []\n","\n","  neg, pos = np.sum(y_test==0), np.sum(y_test==1)\n","  initial_bias = np.log([pos/neg])\n","  total = neg + pos\n","  \n","  class_weights = class_weight.compute_class_weight('balanced',  np.unique(y_test), y_test)\n","  temp_class_weights = dict(enumerate(class_weights))\n","  min_value = min(temp_class_weights.values())\n","  class_weights = {i: temp_class_weights[i] / min_value for i in temp_class_weights}\n","\n","  for fold_num in range(len(x_train_list)):\n","    print(\"Using fold %s/%s\" %(fold_num+1, number_of_folds))\n","    x_train_fold = x_train[x_train_list[fold_num]]\n","    y_train_fold = y_train[y_train_list[fold_num]]\n","    x_valid_fold = x_train[x_valid_list[fold_num]]\n","    y_valid_fold = y_train[y_valid_list[fold_num]]\n","\n","    temp_model_file = model_path  = os.path.join(model_folder, \"fold%s.h5\" %fold_num)\n","\n","    model = train_model_on_fold(x_train_fold, y_train_fold,x_valid_fold, y_valid_fold, model_path=temp_model_folder, \n","                            input_len=150, num_epoch=20, batchsize=128, func = sequence_model, class_weights=class_weights, output_bias = initial_bias)\n","    \n","    if fold_num == 0:\n","      print(model.summary())\n","      plot_model(model, show_shapes=True, show_layer_names=True,rankdir=\"TB\")\n","\n","    print(\"Finish training fold %d\" % (fold_num+1))\n","    print('testing the model')\n","    score = model.evaluate(x_test_seq, y_test)\n","\n","    for i in range(len(model.metrics_names)):\n","        print(str(model.metrics_names[i]) + \": \" + str(score[i]))\n","\n","    acc_per_fold.append(score[1] * 100)\n","    loss_per_fold.append(score[0])\n","    models_path.append(model_path)\n","\n","    model.save(model_path)\n","\n","  print('Average scores for all folds:')\n","  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","  print(f'> Loss: {np.mean(loss_per_fold)}')\n","\n","  if number_of_folds == 1:\n","    return model\n","  \n","  return None "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvGZ7hsF6cP2","colab_type":"text"},"source":["Run the different functions"]},{"cell_type":"code","metadata":{"id":"iR06nRVA35mX","colab_type":"code","colab":{}},"source":["# Train the NN on the scWGBS data using 5 folds \n","scgwbs_data = r\"dataset/scwgbs_crc01_dataset/scwgbs1_crc01.pkl\"\n","models_folder=\"./models/folds_models\"\n","zhou_all_data = r\"dataset/covariance/bulk_prone_0.0153/bulk.pkl\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XknwN_9VNmQ8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597857549738,"user_tz":-180,"elapsed":13944564,"user":{"displayName":"dror bar","photoUrl":"","userId":"08597478424783230611"}},"outputId":"a76cade8-0e34-407c-c30c-a7b221c5dcc9"},"source":["# Train\n","model = train_model(data_path=zhou_all_data, model_folder=models_folder, temp_model_folder=\"./models/temp/weight.h5\", input_len=150, number_of_folds=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading data\n","Using fold 1/5\n","fitting the model\n","Train on 3037596 samples, validate on 759400 samples\n","Epoch 1/30\n","3037440/3037596 [============================>.] - ETA: 0s - loss: 0.4802 - acc: 0.8491 - recall_TP: 0.8543 - recall_TN: 0.8471\n","Epoch 00001: val_loss improved from inf to 0.29310, saving model to ./models/temp/weight.h5\n","3037596/3037596 [==============================] - 223s 73us/sample - loss: 0.4802 - acc: 0.8491 - recall_TP: 0.8543 - recall_TN: 0.8471 - val_loss: 0.2931 - val_acc: 0.8791 - val_recall_TP: 0.8429 - val_recall_TN: 0.8931\n","Epoch 2/30\n","3036928/3037596 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8587 - recall_TP: 0.8726 - recall_TN: 0.8533\n","Epoch 00002: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 222s 73us/sample - loss: 0.4508 - acc: 0.8587 - recall_TP: 0.8726 - recall_TN: 0.8533 - val_loss: 0.3243 - val_acc: 0.8617 - val_recall_TP: 0.9027 - val_recall_TN: 0.8459\n","Epoch 3/30\n","3037056/3037596 [============================>.] - ETA: 0s - loss: 0.4455 - acc: 0.8603 - recall_TP: 0.8764 - recall_TN: 0.8541\n","Epoch 00003: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4455 - acc: 0.8603 - recall_TP: 0.8764 - recall_TN: 0.8541 - val_loss: 0.3214 - val_acc: 0.8566 - val_recall_TP: 0.9138 - val_recall_TN: 0.8345\n","Epoch 4/30\n","3037056/3037596 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8616 - recall_TP: 0.8776 - recall_TN: 0.8554\n","Epoch 00004: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4430 - acc: 0.8616 - recall_TP: 0.8776 - recall_TN: 0.8554 - val_loss: 0.3142 - val_acc: 0.8685 - val_recall_TP: 0.8955 - val_recall_TN: 0.8581\n","Epoch 5/30\n","3036928/3037596 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8625 - recall_TP: 0.8772 - recall_TN: 0.8568\n","Epoch 00005: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4416 - acc: 0.8625 - recall_TP: 0.8772 - recall_TN: 0.8568 - val_loss: 0.3091 - val_acc: 0.8663 - val_recall_TP: 0.9000 - val_recall_TN: 0.8532\n","Epoch 6/30\n","3036928/3037596 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8631 - recall_TP: 0.8765 - recall_TN: 0.8579\n","Epoch 00006: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4408 - acc: 0.8631 - recall_TP: 0.8765 - recall_TN: 0.8579 - val_loss: 0.3134 - val_acc: 0.8713 - val_recall_TP: 0.8913 - val_recall_TN: 0.8635\n","Epoch 7/30\n","3036800/3037596 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8637 - recall_TP: 0.8754 - recall_TN: 0.8592\n","Epoch 00007: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4411 - acc: 0.8637 - recall_TP: 0.8754 - recall_TN: 0.8591 - val_loss: 0.3214 - val_acc: 0.8660 - val_recall_TP: 0.9020 - val_recall_TN: 0.8520\n","Epoch 8/30\n","3037568/3037596 [============================>.] - ETA: 0s - loss: 0.4414 - acc: 0.8638 - recall_TP: 0.8753 - recall_TN: 0.8595\n","Epoch 00008: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4414 - acc: 0.8638 - recall_TP: 0.8753 - recall_TN: 0.8595 - val_loss: 0.3214 - val_acc: 0.8693 - val_recall_TP: 0.8976 - val_recall_TN: 0.8584\n","Epoch 9/30\n","3037056/3037596 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.8640 - recall_TP: 0.8752 - recall_TN: 0.8597\n","Epoch 00009: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4412 - acc: 0.8640 - recall_TP: 0.8752 - recall_TN: 0.8597 - val_loss: 0.3205 - val_acc: 0.8702 - val_recall_TP: 0.8929 - val_recall_TN: 0.8614\n","Epoch 10/30\n","3036928/3037596 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8642 - recall_TP: 0.8748 - recall_TN: 0.8601\n","Epoch 00010: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4418 - acc: 0.8642 - recall_TP: 0.8748 - recall_TN: 0.8601 - val_loss: 0.3180 - val_acc: 0.8634 - val_recall_TP: 0.8992 - val_recall_TN: 0.8495\n","Epoch 11/30\n","3037056/3037596 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8640 - recall_TP: 0.8741 - recall_TN: 0.8601\n","Epoch 00011: val_loss did not improve from 0.29310\n","3037596/3037596 [==============================] - 221s 73us/sample - loss: 0.4417 - acc: 0.8640 - recall_TP: 0.8741 - recall_TN: 0.8601 - val_loss: 0.3154 - val_acc: 0.8673 - val_recall_TP: 0.8974 - val_recall_TN: 0.8557\n","Epoch 00011: early stopping\n","Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input (InputLayer)           [(None, 150, 4)]          0         \n","_________________________________________________________________\n","conv1 (Conv1D)               (None, 148, 90)           1170      \n","_________________________________________________________________\n","pool1 (MaxPooling1D)         (None, 147, 90)           0         \n","_________________________________________________________________\n","drop1 (Dropout)              (None, 147, 90)           0         \n","_________________________________________________________________\n","conv2 (Conv1D)               (None, 143, 100)          45100     \n","_________________________________________________________________\n","max_pooling1d (MaxPooling1D) (None, 142, 100)          0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 142, 100)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 14200)             0         \n","_________________________________________________________________\n","hidden1 (Dense)              (None, 500)               7100500   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 500)               0         \n","_________________________________________________________________\n","hidden2 (Dense)              (None, 250)               125250    \n","_________________________________________________________________\n","output (Dense)               (None, 1)                 251       \n","=================================================================\n","Total params: 7,272,271\n","Trainable params: 7,272,271\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Finish training fold 1\n","testing the model\n","1059338/1059338 [==============================] - 68s 65us/sample - loss: 0.3126 - acc: 0.8691 - recall_TP: 0.8827 - recall_TN: 0.8692\n","loss: 0.3126398167399864\n","acc: 0.86906916\n","recall_TP: 0.8826921\n","recall_TN: 0.8691783\n","Using fold 2/5\n","fitting the model\n","Train on 3037597 samples, validate on 759399 samples\n","Epoch 1/30\n","3037184/3037597 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.8474 - recall_TP: 0.8520 - recall_TN: 0.8456\n","Epoch 00001: val_loss improved from inf to 0.29190, saving model to ./models/temp/weight.h5\n","3037597/3037597 [==============================] - 222s 73us/sample - loss: 0.4847 - acc: 0.8474 - recall_TP: 0.8520 - recall_TN: 0.8456 - val_loss: 0.2919 - val_acc: 0.8782 - val_recall_TP: 0.8455 - val_recall_TN: 0.8909\n","Epoch 2/30\n","3037312/3037597 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8571 - recall_TP: 0.8699 - recall_TN: 0.8522\n","Epoch 00002: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4553 - acc: 0.8571 - recall_TP: 0.8699 - recall_TN: 0.8521 - val_loss: 0.2943 - val_acc: 0.8750 - val_recall_TP: 0.8682 - val_recall_TN: 0.8777\n","Epoch 3/30\n","3037440/3037597 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8582 - recall_TP: 0.8745 - recall_TN: 0.8519\n","Epoch 00003: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4502 - acc: 0.8582 - recall_TP: 0.8745 - recall_TN: 0.8519 - val_loss: 0.2951 - val_acc: 0.8735 - val_recall_TP: 0.8768 - val_recall_TN: 0.8722\n","Epoch 4/30\n","3036928/3037597 [============================>.] - ETA: 0s - loss: 0.4479 - acc: 0.8596 - recall_TP: 0.8750 - recall_TN: 0.8537\n","Epoch 00004: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4479 - acc: 0.8596 - recall_TP: 0.8750 - recall_TN: 0.8537 - val_loss: 0.3285 - val_acc: 0.8596 - val_recall_TP: 0.9079 - val_recall_TN: 0.8409\n","Epoch 5/30\n","3037568/3037597 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8604 - recall_TP: 0.8753 - recall_TN: 0.8546\n","Epoch 00005: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4467 - acc: 0.8604 - recall_TP: 0.8753 - recall_TN: 0.8546 - val_loss: 0.2987 - val_acc: 0.8707 - val_recall_TP: 0.8893 - val_recall_TN: 0.8636\n","Epoch 6/30\n","3037440/3037597 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.8608 - recall_TP: 0.8753 - recall_TN: 0.8552\n","Epoch 00006: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4461 - acc: 0.8608 - recall_TP: 0.8753 - recall_TN: 0.8552 - val_loss: 0.3073 - val_acc: 0.8688 - val_recall_TP: 0.8944 - val_recall_TN: 0.8589\n","Epoch 7/30\n","3037312/3037597 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.8612 - recall_TP: 0.8748 - recall_TN: 0.8560\n","Epoch 00007: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4456 - acc: 0.8612 - recall_TP: 0.8749 - recall_TN: 0.8560 - val_loss: 0.2963 - val_acc: 0.8753 - val_recall_TP: 0.8789 - val_recall_TN: 0.8740\n","Epoch 8/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4457 - acc: 0.8614 - recall_TP: 0.8745 - recall_TN: 0.8564\n","Epoch 00008: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4457 - acc: 0.8614 - recall_TP: 0.8745 - recall_TN: 0.8564 - val_loss: 0.3015 - val_acc: 0.8717 - val_recall_TP: 0.8859 - val_recall_TN: 0.8662\n","Epoch 9/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4460 - acc: 0.8615 - recall_TP: 0.8740 - recall_TN: 0.8567\n","Epoch 00009: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4460 - acc: 0.8615 - recall_TP: 0.8740 - recall_TN: 0.8567 - val_loss: 0.3034 - val_acc: 0.8742 - val_recall_TP: 0.8793 - val_recall_TN: 0.8722\n","Epoch 10/30\n","3037184/3037597 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8617 - recall_TP: 0.8736 - recall_TN: 0.8571\n","Epoch 00010: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4458 - acc: 0.8617 - recall_TP: 0.8736 - recall_TN: 0.8571 - val_loss: 0.3013 - val_acc: 0.8799 - val_recall_TP: 0.8635 - val_recall_TN: 0.8863\n","Epoch 11/30\n","3036800/3037597 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8617 - recall_TP: 0.8732 - recall_TN: 0.8572\n","Epoch 00011: val_loss did not improve from 0.29190\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4463 - acc: 0.8617 - recall_TP: 0.8732 - recall_TN: 0.8572 - val_loss: 0.3098 - val_acc: 0.8720 - val_recall_TP: 0.8812 - val_recall_TN: 0.8685\n","Epoch 00011: early stopping\n","Finish training fold 2\n","testing the model\n","1059338/1059338 [==============================] - 71s 67us/sample - loss: 0.3098 - acc: 0.8723 - recall_TP: 0.8618 - recall_TN: 0.8817\n","loss: 0.3097585722938236\n","acc: 0.8722901\n","recall_TP: 0.86182946\n","recall_TN: 0.8816772\n","Using fold 3/5\n","fitting the model\n","Train on 3037597 samples, validate on 759399 samples\n","Epoch 1/30\n","3037184/3037597 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8465 - recall_TP: 0.8499 - recall_TN: 0.8452\n","Epoch 00001: val_loss improved from inf to 0.29508, saving model to ./models/temp/weight.h5\n","3037597/3037597 [==============================] - 221s 73us/sample - loss: 0.4883 - acc: 0.8465 - recall_TP: 0.8499 - recall_TN: 0.8452 - val_loss: 0.2951 - val_acc: 0.8766 - val_recall_TP: 0.8476 - val_recall_TN: 0.8878\n","Epoch 2/30\n","3037568/3037597 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.8567 - recall_TP: 0.8665 - recall_TN: 0.8529\n","Epoch 00002: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4585 - acc: 0.8567 - recall_TP: 0.8665 - recall_TN: 0.8529 - val_loss: 0.2968 - val_acc: 0.8724 - val_recall_TP: 0.8731 - val_recall_TN: 0.8721\n","Epoch 3/30\n","3037184/3037597 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.8586 - recall_TP: 0.8715 - recall_TN: 0.8536\n","Epoch 00003: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4523 - acc: 0.8586 - recall_TP: 0.8715 - recall_TN: 0.8536 - val_loss: 0.3014 - val_acc: 0.8693 - val_recall_TP: 0.8884 - val_recall_TN: 0.8620\n","Epoch 4/30\n","3037184/3037597 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8595 - recall_TP: 0.8727 - recall_TN: 0.8544\n","Epoch 00004: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4502 - acc: 0.8595 - recall_TP: 0.8727 - recall_TN: 0.8544 - val_loss: 0.3066 - val_acc: 0.8680 - val_recall_TP: 0.8916 - val_recall_TN: 0.8589\n","Epoch 5/30\n","3037312/3037597 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.8601 - recall_TP: 0.8734 - recall_TN: 0.8550\n","Epoch 00005: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4493 - acc: 0.8601 - recall_TP: 0.8734 - recall_TN: 0.8550 - val_loss: 0.3110 - val_acc: 0.8645 - val_recall_TP: 0.8964 - val_recall_TN: 0.8521\n","Epoch 6/30\n","3037184/3037597 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.8598 - recall_TP: 0.8738 - recall_TN: 0.8544\n","Epoch 00006: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4494 - acc: 0.8598 - recall_TP: 0.8739 - recall_TN: 0.8544 - val_loss: 0.3245 - val_acc: 0.8636 - val_recall_TP: 0.9012 - val_recall_TN: 0.8490\n","Epoch 7/30\n","3037568/3037597 [============================>.] - ETA: 0s - loss: 0.4491 - acc: 0.8605 - recall_TP: 0.8733 - recall_TN: 0.8554\n","Epoch 00007: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4491 - acc: 0.8605 - recall_TP: 0.8733 - recall_TN: 0.8554 - val_loss: 0.3419 - val_acc: 0.8608 - val_recall_TP: 0.9074 - val_recall_TN: 0.8428\n","Epoch 8/30\n","3037440/3037597 [============================>.] - ETA: 0s - loss: 0.4495 - acc: 0.8602 - recall_TP: 0.8733 - recall_TN: 0.8551\n","Epoch 00008: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4495 - acc: 0.8602 - recall_TP: 0.8733 - recall_TN: 0.8551 - val_loss: 0.3323 - val_acc: 0.8643 - val_recall_TP: 0.9012 - val_recall_TN: 0.8500\n","Epoch 9/30\n","3037568/3037597 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8602 - recall_TP: 0.8737 - recall_TN: 0.8549\n","Epoch 00009: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4500 - acc: 0.8602 - recall_TP: 0.8737 - recall_TN: 0.8549 - val_loss: 0.3043 - val_acc: 0.8743 - val_recall_TP: 0.8794 - val_recall_TN: 0.8723\n","Epoch 10/30\n","3036928/3037597 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8603 - recall_TP: 0.8737 - recall_TN: 0.8550\n","Epoch 00010: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4496 - acc: 0.8603 - recall_TP: 0.8737 - recall_TN: 0.8550 - val_loss: 0.3405 - val_acc: 0.8584 - val_recall_TP: 0.9108 - val_recall_TN: 0.8381\n","Epoch 11/30\n","3037312/3037597 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.8602 - recall_TP: 0.8726 - recall_TN: 0.8554\n","Epoch 00011: val_loss did not improve from 0.29508\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4504 - acc: 0.8602 - recall_TP: 0.8726 - recall_TN: 0.8554 - val_loss: 0.3345 - val_acc: 0.8642 - val_recall_TP: 0.9007 - val_recall_TN: 0.8500\n","Epoch 00011: early stopping\n","Finish training fold 3\n","testing the model\n","1059338/1059338 [==============================] - 71s 67us/sample - loss: 0.3288 - acc: 0.8672 - recall_TP: 0.8846 - recall_TN: 0.8647\n","loss: 0.3288381261963449\n","acc: 0.8672426\n","recall_TP: 0.88457155\n","recall_TN: 0.8646569\n","Using fold 4/5\n","fitting the model\n","Train on 3037597 samples, validate on 759399 samples\n","Epoch 1/30\n","3036928/3037597 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.8471 - recall_TP: 0.8531 - recall_TN: 0.8448\n","Epoch 00001: val_loss improved from inf to 0.31342, saving model to ./models/temp/weight.h5\n","3037597/3037597 [==============================] - 221s 73us/sample - loss: 0.4847 - acc: 0.8471 - recall_TP: 0.8531 - recall_TN: 0.8448 - val_loss: 0.3134 - val_acc: 0.8639 - val_recall_TP: 0.8888 - val_recall_TN: 0.8543\n","Epoch 2/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4534 - acc: 0.8582 - recall_TP: 0.8709 - recall_TN: 0.8532\n","Epoch 00002: val_loss improved from 0.31342 to 0.31008, saving model to ./models/temp/weight.h5\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4534 - acc: 0.8582 - recall_TP: 0.8709 - recall_TN: 0.8532 - val_loss: 0.3101 - val_acc: 0.8673 - val_recall_TP: 0.8886 - val_recall_TN: 0.8591\n","Epoch 3/30\n","3037184/3037597 [============================>.] - ETA: 0s - loss: 0.4487 - acc: 0.8605 - recall_TP: 0.8725 - recall_TN: 0.8559\n","Epoch 00003: val_loss improved from 0.31008 to 0.30354, saving model to ./models/temp/weight.h5\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4487 - acc: 0.8605 - recall_TP: 0.8725 - recall_TN: 0.8559 - val_loss: 0.3035 - val_acc: 0.8689 - val_recall_TP: 0.8876 - val_recall_TN: 0.8617\n","Epoch 4/30\n","3037440/3037597 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8618 - recall_TP: 0.8733 - recall_TN: 0.8572\n","Epoch 00004: val_loss improved from 0.30354 to 0.29539, saving model to ./models/temp/weight.h5\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4465 - acc: 0.8618 - recall_TP: 0.8733 - recall_TN: 0.8572 - val_loss: 0.2954 - val_acc: 0.8774 - val_recall_TP: 0.8703 - val_recall_TN: 0.8801\n","Epoch 5/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8619 - recall_TP: 0.8738 - recall_TN: 0.8573\n","Epoch 00005: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 218s 72us/sample - loss: 0.4458 - acc: 0.8619 - recall_TP: 0.8738 - recall_TN: 0.8573 - val_loss: 0.2961 - val_acc: 0.8740 - val_recall_TP: 0.8781 - val_recall_TN: 0.8724\n","Epoch 6/30\n","3036800/3037597 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.8617 - recall_TP: 0.8757 - recall_TN: 0.8563\n","Epoch 00006: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 218s 72us/sample - loss: 0.4456 - acc: 0.8617 - recall_TP: 0.8757 - recall_TN: 0.8563 - val_loss: 0.3109 - val_acc: 0.8714 - val_recall_TP: 0.8875 - val_recall_TN: 0.8651\n","Epoch 7/30\n","3037568/3037597 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.8614 - recall_TP: 0.8760 - recall_TN: 0.8558\n","Epoch 00007: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4461 - acc: 0.8614 - recall_TP: 0.8760 - recall_TN: 0.8558 - val_loss: 0.3036 - val_acc: 0.8707 - val_recall_TP: 0.8865 - val_recall_TN: 0.8646\n","Epoch 8/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4469 - acc: 0.8616 - recall_TP: 0.8751 - recall_TN: 0.8564\n","Epoch 00008: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4469 - acc: 0.8616 - recall_TP: 0.8751 - recall_TN: 0.8564 - val_loss: 0.3048 - val_acc: 0.8743 - val_recall_TP: 0.8750 - val_recall_TN: 0.8740\n","Epoch 9/30\n","3037312/3037597 [============================>.] - ETA: 0s - loss: 0.4470 - acc: 0.8610 - recall_TP: 0.8764 - recall_TN: 0.8551\n","Epoch 00009: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4470 - acc: 0.8610 - recall_TP: 0.8763 - recall_TN: 0.8551 - val_loss: 0.3214 - val_acc: 0.8720 - val_recall_TP: 0.8825 - val_recall_TN: 0.8679\n","Epoch 10/30\n","3036928/3037597 [============================>.] - ETA: 0s - loss: 0.4469 - acc: 0.8617 - recall_TP: 0.8755 - recall_TN: 0.8564\n","Epoch 00010: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 220s 73us/sample - loss: 0.4469 - acc: 0.8617 - recall_TP: 0.8755 - recall_TN: 0.8564 - val_loss: 0.3003 - val_acc: 0.8729 - val_recall_TP: 0.8805 - val_recall_TN: 0.8700\n","Epoch 11/30\n","3037568/3037597 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8618 - recall_TP: 0.8750 - recall_TN: 0.8567\n","Epoch 00011: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4475 - acc: 0.8618 - recall_TP: 0.8750 - recall_TN: 0.8567 - val_loss: 0.3224 - val_acc: 0.8620 - val_recall_TP: 0.9033 - val_recall_TN: 0.8460\n","Epoch 12/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4473 - acc: 0.8619 - recall_TP: 0.8749 - recall_TN: 0.8568\n","Epoch 00012: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 218s 72us/sample - loss: 0.4473 - acc: 0.8619 - recall_TP: 0.8748 - recall_TN: 0.8568 - val_loss: 0.3218 - val_acc: 0.8675 - val_recall_TP: 0.8922 - val_recall_TN: 0.8579\n","Epoch 13/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8619 - recall_TP: 0.8746 - recall_TN: 0.8570\n","Epoch 00013: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 218s 72us/sample - loss: 0.4474 - acc: 0.8619 - recall_TP: 0.8746 - recall_TN: 0.8570 - val_loss: 0.3292 - val_acc: 0.8632 - val_recall_TP: 0.9015 - val_recall_TN: 0.8484\n","Epoch 14/30\n","3036928/3037597 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.8621 - recall_TP: 0.8743 - recall_TN: 0.8574\n","Epoch 00014: val_loss did not improve from 0.29539\n","3037597/3037597 [==============================] - 219s 72us/sample - loss: 0.4472 - acc: 0.8621 - recall_TP: 0.8743 - recall_TN: 0.8574 - val_loss: 0.3254 - val_acc: 0.8634 - val_recall_TP: 0.9009 - val_recall_TN: 0.8489\n","Epoch 00014: early stopping\n","Finish training fold 4\n","testing the model\n","1059338/1059338 [==============================] - 74s 69us/sample - loss: 0.3218 - acc: 0.8666 - recall_TP: 0.8863 - recall_TN: 0.8636\n","loss: 0.32180949265705994\n","acc: 0.86663747\n","recall_TP: 0.88625157\n","recall_TN: 0.86360246\n","Using fold 5/5\n","fitting the model\n","Train on 3037597 samples, validate on 759399 samples\n","Epoch 1/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4827 - acc: 0.8477 - recall_TP: 0.8529 - recall_TN: 0.8457\n","Epoch 00001: val_loss improved from inf to 0.32165, saving model to ./models/temp/weight.h5\n","3037597/3037597 [==============================] - 222s 73us/sample - loss: 0.4827 - acc: 0.8477 - recall_TP: 0.8529 - recall_TN: 0.8456 - val_loss: 0.3216 - val_acc: 0.8766 - val_recall_TP: 0.8522 - val_recall_TN: 0.8861\n","Epoch 2/30\n","3037184/3037597 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8587 - recall_TP: 0.8709 - recall_TN: 0.8539\n","Epoch 00002: val_loss improved from 0.32165 to 0.31126, saving model to ./models/temp/weight.h5\n","3037597/3037597 [==============================] - 221s 73us/sample - loss: 0.4522 - acc: 0.8587 - recall_TP: 0.8709 - recall_TN: 0.8539 - val_loss: 0.3113 - val_acc: 0.8745 - val_recall_TP: 0.8722 - val_recall_TN: 0.8754\n","Epoch 3/30\n","3036928/3037597 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8603 - recall_TP: 0.8741 - recall_TN: 0.8550\n","Epoch 00003: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 73us/sample - loss: 0.4463 - acc: 0.8603 - recall_TP: 0.8741 - recall_TN: 0.8550 - val_loss: 0.3220 - val_acc: 0.8662 - val_recall_TP: 0.8956 - val_recall_TN: 0.8549\n","Epoch 4/30\n","3037568/3037597 [============================>.] - ETA: 0s - loss: 0.4439 - acc: 0.8612 - recall_TP: 0.8759 - recall_TN: 0.8554\n","Epoch 00004: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4439 - acc: 0.8612 - recall_TP: 0.8759 - recall_TN: 0.8554 - val_loss: 0.3138 - val_acc: 0.8741 - val_recall_TP: 0.8808 - val_recall_TN: 0.8715\n","Epoch 5/30\n","3037312/3037597 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8615 - recall_TP: 0.8770 - recall_TN: 0.8555\n","Epoch 00005: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4427 - acc: 0.8615 - recall_TP: 0.8770 - recall_TN: 0.8555 - val_loss: 0.3242 - val_acc: 0.8639 - val_recall_TP: 0.9024 - val_recall_TN: 0.8490\n","Epoch 6/30\n","3037312/3037597 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8621 - recall_TP: 0.8763 - recall_TN: 0.8565\n","Epoch 00006: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4422 - acc: 0.8621 - recall_TP: 0.8763 - recall_TN: 0.8565 - val_loss: 0.3280 - val_acc: 0.8545 - val_recall_TP: 0.9171 - val_recall_TN: 0.8302\n","Epoch 7/30\n","3037440/3037597 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8623 - recall_TP: 0.8764 - recall_TN: 0.8568\n","Epoch 00007: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4416 - acc: 0.8623 - recall_TP: 0.8764 - recall_TN: 0.8568 - val_loss: 0.3277 - val_acc: 0.8622 - val_recall_TP: 0.9055 - val_recall_TN: 0.8454\n","Epoch 8/30\n","3037312/3037597 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8626 - recall_TP: 0.8759 - recall_TN: 0.8575\n","Epoch 00008: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 73us/sample - loss: 0.4419 - acc: 0.8626 - recall_TP: 0.8759 - recall_TN: 0.8575 - val_loss: 0.3234 - val_acc: 0.8667 - val_recall_TP: 0.8946 - val_recall_TN: 0.8560\n","Epoch 9/30\n","3037056/3037597 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8631 - recall_TP: 0.8758 - recall_TN: 0.8582\n","Epoch 00009: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4417 - acc: 0.8631 - recall_TP: 0.8758 - recall_TN: 0.8581 - val_loss: 0.3370 - val_acc: 0.8590 - val_recall_TP: 0.9096 - val_recall_TN: 0.8395\n","Epoch 10/30\n","3036928/3037597 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8627 - recall_TP: 0.8757 - recall_TN: 0.8577\n","Epoch 00010: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4426 - acc: 0.8627 - recall_TP: 0.8757 - recall_TN: 0.8577 - val_loss: 0.3475 - val_acc: 0.8573 - val_recall_TP: 0.9128 - val_recall_TN: 0.8358\n","Epoch 11/30\n","3036928/3037597 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8627 - recall_TP: 0.8750 - recall_TN: 0.8579\n","Epoch 00011: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4429 - acc: 0.8627 - recall_TP: 0.8750 - recall_TN: 0.8579 - val_loss: 0.3500 - val_acc: 0.8605 - val_recall_TP: 0.9081 - val_recall_TN: 0.8420\n","Epoch 12/30\n","3036800/3037597 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.8633 - recall_TP: 0.8745 - recall_TN: 0.8589\n","Epoch 00012: val_loss did not improve from 0.31126\n","3037597/3037597 [==============================] - 220s 72us/sample - loss: 0.4424 - acc: 0.8633 - recall_TP: 0.8745 - recall_TN: 0.8590 - val_loss: 0.3367 - val_acc: 0.8648 - val_recall_TP: 0.9015 - val_recall_TN: 0.8506\n","Epoch 00012: early stopping\n","Finish training fold 5\n","testing the model\n","1059338/1059338 [==============================] - 71s 67us/sample - loss: 0.3336 - acc: 0.8678 - recall_TP: 0.8840 - recall_TN: 0.8658\n","loss: 0.3335853567217157\n","acc: 0.8678401\n","recall_TP: 0.88399124\n","recall_TN: 0.8658103\n","Average scores for all folds:\n","> Accuracy: 86.86158776283264 (+- 0.20053461951799514)\n","> Loss: 0.3213262729217861\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"suzJWxJ4SWb0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1597858917051,"user_tz":-180,"elapsed":782864,"user":{"displayName":"dror bar","photoUrl":"","userId":"08597478424783230611"}},"outputId":"e81b70ba-8ab5-4fb8-c7d7-bbde66ce08e8"},"source":["# Test\n","_,_,_,_, x_test_seq, y_test,_,_ = load_train_validate_test_data(path_to_data=zhou_all_data, input_len=150, kfold=1, only_test=True)\n","\n","models = load_models(models_folder)\n","get_scores(models,x_test_seq, y_test)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1059338/1059338 [==============================] - 77s 73us/sample - loss: 0.3126 - acc: 0.8691 - recall_TP: 0.8827 - recall_TN: 0.8692\n","1059338/1059338 [==============================] - 78s 74us/sample - loss: 0.3098 - acc: 0.8723 - recall_TP: 0.8618 - recall_TN: 0.8817\n","1059338/1059338 [==============================] - 77s 72us/sample - loss: 0.3288 - acc: 0.8672 - recall_TP: 0.8846 - recall_TN: 0.8647\n","1059338/1059338 [==============================] - 78s 73us/sample - loss: 0.3218 - acc: 0.8666 - recall_TP: 0.8863 - recall_TN: 0.8636\n","1059338/1059338 [==============================] - 76s 72us/sample - loss: 0.3336 - acc: 0.8678 - recall_TP: 0.8840 - recall_TN: 0.8658\n","Average scores for all folds:\n","> Accuracy: 86.86158776283264 (+- 0.20053461951799514)\n","> Loss: 0.3213262729217861\n",">Truee accuracy using majority vote: 0.872701630641023\n","> AUC: 0.944390955958202\n","> Recall: 0.874245318088011\n"],"name":"stdout"}]}]}