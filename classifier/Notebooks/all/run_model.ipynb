{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_model.ipynb","provenance":[{"file_id":"1qysG6N3-gJJVB6B7BHO3zuP-0sCnDUOr","timestamp":1596352204324},{"file_id":"1ziwyTEY4r7YvqKNegyQMKPHy1_Szf20R","timestamp":1595938496751}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d29_mG9z6PXb"},"source":["Train a model to predict complete loss of methylation or partial loss using a sequence"]},{"cell_type":"code","metadata":{"id":"1wz-ud5Hav1O","executionInfo":{"status":"ok","timestamp":1600316006957,"user_tz":-180,"elapsed":24661,"user":{"displayName":"dror bar","photoUrl":"","userId":"08597478424783230611"}},"outputId":"3921ff8c-d99a-47d8-f50d-cb02fb3b47b8","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vjO8Tu5MbI-0","executionInfo":{"status":"ok","timestamp":1600316006967,"user_tz":-180,"elapsed":24637,"user":{"displayName":"dror bar","photoUrl":"","userId":"08597478424783230611"}},"outputId":"0944f5e2-b92b-49c3-a008-325f89033a5b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cd /gdrive/My\\ Drive/nn "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive/nn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5BI8sNFMb0Lp","executionInfo":{"status":"ok","timestamp":1600316014235,"user_tz":-180,"elapsed":31502,"user":{"displayName":"dror bar","photoUrl":"","userId":"08597478424783230611"}},"outputId":"8b1e6b80-aecc-49f8-8f6e-94881adef43d","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["%tensorflow_version 1.x\n","import argparse\n","import os\n","import pickle\n","import sys\n","import glob\n","\n","import numpy as np\n","\n","np.random.seed(7)  # for reproducibility\n","\n","import tensorflow as tf\n","tf.random.set_random_seed(5005)\n","\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.utils import class_weight\n","\n","\n","from tensorflow.python.keras.models import Model, load_model\n","from tensorflow.python.keras.layers import Input\n","from tensorflow.python.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.python.keras.layers.convolutional import Conv1D\n","from tensorflow.python.keras.layers.pooling import MaxPooling1D\n","from tensorflow.python.keras.layers.pooling import AveragePooling1D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n","import tensorflow.python.keras.backend as K\n","from keras import regularizers\n","from tensorflow.python.keras.utils import plot_model \n","\n","sys.path.append(\".\")\n","import utils\n","from utils import *\n","\n","l2_lam = 5e-07 \n","l1_lam = 1e-08 "],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lHcO6wobeAjx"},"source":["def train_model_on_fold(x_train, y_train, x_test,y_test, input_len,\n","                        num_epoch, batchsize, func,model_path, class_weights, output_bias=None):\n","  \"\"\"\n","  Train a model to using the train data to predict the test data\n","  :param x_train: The train dataset \n","  :param y_train: The train labels\n","  :param x_test: The test dataset\n","  :param y_test: The test labels\n","  :param input_len: The length of the input\n","  :param num_epoch: Number of epoches \n","  :param batchsize: The batchsize \n","  :param func: The model function to use \n","  :param model_path: The path to save the model from run to run\n","  :return: The model after fitting\n","  \"\"\"\n","  model = func(input_len, output_bias=output_bias)\n","  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n","  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', recall_TP,recall_TN])\n","  checkpointer = ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True)\n","  earlystopper = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1)\n","    \n","  print('fitting the model')          \n","  history = model.fit(x_train, y_train, epochs=num_epoch, batch_size=batchsize,\n","                      validation_data=(x_test, y_test), verbose=1,\n","                      callbacks=[checkpointer, earlystopper, ], class_weight=class_weights)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hy29UlwGoN35"},"source":["def sequence_model(input_len, output_bias=None):\n","  \"\"\"\n","  Buld a model to predict a sequence information \n","  :param input_len: The length of the input\n","  \"\"\"\n","  K.clear_session()\n","  tf.random.set_random_seed(5005)\n","\n","  if output_bias:\n","    output_bias = tf.keras.initializers.Constant(output_bias)\n","\n","  input_node = Input(shape=(input_len, 4), name=\"input\")\n","  conv1 = Conv1D(filters=90, kernel_size=3, padding='valid', activation=\"relu\", name=\"conv1\",kernel_regularizer=regularizers.l2(l2_lam))(input_node)\n","  pool1 = MaxPooling1D(pool_size=2, strides=1, name=\"pool1\")(conv1)\n","  drop1 = Dropout(0.25, name=\"drop1\")(pool1)\n","\n","  conv2 = Conv1D(filters=100, kernel_size=5, padding='valid', activation=\"relu\", name=\"conv2\", kernel_regularizer=regularizers.l2(l2_lam))(drop1)\n","  pool2 = MaxPooling1D(pool_size=2, strides=1)(conv2)\n","  drop2 = Dropout(0.25)(pool2)\n","  flat = Flatten()(drop2)\n","\n","  hidden1 = Dense(500, activation='relu', name=\"hidden1\",kernel_regularizer=regularizers.l1(l1_lam))(flat)\n","  drop3 = Dropout(0.5)(hidden1)\n","  hidden2 = Dense(250, activation='relu', name=\"hidden2\",kernel_regularizer=regularizers.l1(l1_lam))(drop3)\n","\n","  output = Dense(1, activation='sigmoid', name=\"output\", bias_initializer=output_bias)(hidden2)\n","  model = Model(inputs=[input_node], outputs=output)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Qi9WBFs-vub"},"source":["def train_model(data_path, model_folder=\"./models/folds_models\", temp_model_folder=\"./models/temp/weight.h5\", input_len=150, number_of_folds=3):\n","  \"\"\"\n","  Train a model or x models using the cross validation \n","  :param data_path: The path for the dataset\n","  :param model_folder: The final folder to save the models\n","  :param temp_model_folder: A folder to save the models while running\n","  :param input_len: The length of the input\n","  :param number_of_folds: Number of fold to use for the model\n","  :return: The model if we used 1 model(1 fold) or None if more \n","  \"\"\"\n","\n","  print('loading data')\n","  x_train_list, y_train_list, x_valid_list, y_valid_list, x_test_seq, y_test, x_train, y_train = load_train_validate_test_data(data_path, input_len, kfold=number_of_folds)\n","\n","  models_path = []\n","  acc_per_fold = []\n","  loss_per_fold = []\n","\n","  neg, pos = np.sum(y_test==0), np.sum(y_test==1)\n","  initial_bias = np.log([pos/neg])\n","  total = neg + pos\n","  \n","  class_weights = class_weight.compute_class_weight('balanced',  np.unique(y_test), y_test)\n","  temp_class_weights = dict(enumerate(class_weights))\n","  min_value = min(temp_class_weights.values())\n","  class_weights = {i: temp_class_weights[i] / min_value for i in temp_class_weights}\n","\n","  for fold_num in range(len(x_train_list)):\n","    print(\"Using fold %s/%s\" %(fold_num+1, number_of_folds))\n","    x_train_fold = x_train[x_train_list[fold_num]] if number_of_folds != 1 else x_train_list[fold_num]\n","    y_train_fold = y_train[y_train_list[fold_num]] if number_of_folds != 1 else y_train_list[fold_num]\n","    x_valid_fold = x_train[x_valid_list[fold_num]] if number_of_folds != 1 else x_valid_list[fold_num]\n","    y_valid_fold = y_train[y_valid_list[fold_num]] if number_of_folds != 1 else y_valid_list[fold_num]\n","\n","    temp_model_file = model_path  = os.path.join(model_folder, \"fold%s.h5\" %fold_num)\n","\n","    model = train_model_on_fold(x_train_fold, y_train_fold,x_valid_fold, y_valid_fold, model_path=temp_model_folder, \n","                            input_len=150, num_epoch=20, batchsize=128, func = sequence_model, class_weights=class_weights, output_bias = initial_bias)\n","    \n","    if fold_num == 0:\n","      print(model.summary())\n","      plot_model(model, show_shapes=True, show_layer_names=True,rankdir=\"TB\")\n","\n","    print(\"Finish training fold %d\" % (fold_num+1))\n","    print('testing the model')\n","    score = model.evaluate(x_test_seq, y_test)\n","\n","    for i in range(len(model.metrics_names)):\n","        print(str(model.metrics_names[i]) + \": \" + str(score[i]))\n","\n","    acc_per_fold.append(score[1] * 100)\n","    loss_per_fold.append(score[0])\n","    models_path.append(model_path)\n","\n","    model.save(model_path)\n","\n","  print('Average scores for all folds:')\n","  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","  print(f'> Loss: {np.mean(loss_per_fold)}')\n","\n","  if number_of_folds == 1:\n","    return model\n","  \n","  return None "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvGZ7hsF6cP2"},"source":["# Train the two models"]},{"cell_type":"code","metadata":{"id":"iR06nRVA35mX"},"source":["# Train the NN on the scWGBS data using 5 folds \n","bian_data = r\"dataset/bian_crc01_train_test.pkl\"\n","zhou_data = r\"dataset/zhou_train_test.pkl\"\n","\n","models_folder_zhou =\"./models/zhou\"\n","models_folder_bian = \"./models/bian\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XknwN_9VNmQ8"},"source":["# Train zhou model\n","model = train_model(data_path=zhou_data, model_folder=models_folder_zhou, temp_model_folder=\"./models/temp/weight.h5\", input_len=150, number_of_folds=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuxuBT8OsiFX"},"source":["# Train Bian model\n","model = train_model(data_path=bian_data, model_folder=models_folder_bian, temp_model_folder=\"./models/temp/weight.h5\", input_len=150, number_of_folds=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"suzJWxJ4SWb0"},"source":["# Test zhou model\n","_,_,_,_, x_test_seq, y_test,_,_ = load_train_validate_test_data(path_to_data=zhou_data, input_len=150, kfold=1, only_test=True)\n","\n","models = load_models(models_folder_zhou)\n","get_scores(models,x_test_seq, y_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kdrK10chsoUU"},"source":["# Test bian model\n","_,_,_,_, x_test_seq, y_test,_,_ = load_train_validate_test_data(path_to_data=bian_data, input_len=150, kfold=1, only_test=True)\n","\n","models = load_models(models_folder_bian)\n","get_scores(models,x_test_seq, y_test)"],"execution_count":null,"outputs":[]}]}